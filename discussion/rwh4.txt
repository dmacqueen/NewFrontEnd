// discussion/rwh4.txt
// RWH
// 2024.12.6

Agreed [dbm3.txt].  That's a tough paper but there are other ways to explain it that are clearer.  Mind if I give it a try here?

At the highest level, one thing that came out of our lengthy investigations in higher-dimensional type theory (which arose from Voevodsky's proposal of the so-called univalence axiom, a topic for another time) is the use of "synthetic" methods for formulating type theories and for formulating their metatheory.  The term "synthetic" here is precisely that used for the longstanding dichotomy between synthetic geometry (Euclid) and analytic geometry (Descartes).  In the synthetic setting the objects under study (lines, circles, etc in the geometric case) are abstractions in themselves, rather than being constructed out of something, namely sets of points in R^2 in the analytic formulation of, say, a circle as the locus of points equidistant from a given point, etc.  There are many advantages to synthetic methods, namely that you're working at the highest level of abstraction in which a result can be meaningfully expressed and established, leaving aside as much "clutter" as possible.

This viewpoint is natural for PL's, which strive for the same overall goals, namely to isolate abstractions that are of use in coding without resorting to defining them or reducing them to "simpler" terms such as pointers and memory.  So in ML we happily have pairs and lists as "things", and don't have to worry about "pointers" and "dereferencing" and "allocation".  (I've often had students tell me that it took them a long time to realize that what I was writing at the board was in fact executable code, and not just some mathematical abstraction of it!)  As far as "core" concepts are concerned, this has largely been achieved, and the methodology is pretty clear.

But what about "modularity"?  The world continues to ignore it for reasons unknown to me, preferring self-evident horseshit like "objects" and "classes" to structure code, and forever and always failing.  (Their conclusion?  PL's are useless and don't bother about those PL people and their languages.)  But one has to consider, what if I were to design a language from scratch, in a way that from the outset takes account of modularity, rather than viewing it as an afterthought to be bolted on (maybe) some years down the road.  My view, very strongly held at this point, is that the right way, the only way, to proceed is to start with modules and "bolt on" a core language later!  I know this may seem odd, but we've learned a lot since the early days of ML and modules, and I think it's time to rethink it ... using synthetic methods.

Now, before I get into more details, let me say at a high level that we have adopted synthetic methods for both language design and language meta-theory; that paper with Jon is a worked example of it in action.  The meta-theory is particularly cool, being a current high-water mark in using synthetic methods in semantics.  But first things first, let me explain the approach and the technical devices that are used to set it up.

Perhaps another preliminary remark is that it is absolutely necessary to separate "elaboration" of an external source language into an internal type theory that is well-behaved and intrinsically meaningful.  This internal type theory is what I am talking about here.  Now I admit that there is room for variation on what constitutes "elaboration" as distinct from what is present in the internal type theory, but let me say that at a high level issues such as identifier scope resolution, type inference, coercive signature matching, derived forms, are all to be handled during elaboration, and are not themselves part of the internal type theory.  (For example, all coercive aspects of matching are definable in the internal type theory, so the job of the elaborator is to define them and apply them wherever necessary.)  However, matters of type sharing are not eliminable, and must be account for in the internal language itself, because type equality is essential for defining well-typed-ness, even in incredibly simple cases where it is just alpha equivalence, but in more complicated cases where, say, it must take account of sharing specifications that are in scope at a given point.

Alright, so now let's look at an internal type theory for a PL that, of course, starts with modules as constituting the overall structure of the language to which other things are added as appropriate and in a manner to be sensed below.  But here's a very high-level overview; the new concepts that we introduced are to do with sharing specifications and their satisfaction during type checking using what we call a phase distinction between static (compile-time) and dynamic (run-time) aspects of a piece of code.  But I get ahead of myself, so let me start simple with ideas you first put forth in the mid- to late 80's.
The synthetic approach is characterized by saying that we are developing a type theory in which "everything in sight" are modules classified by signatures.  That is, types are signatures ab initio, and their inhabitants are modules.
As with your 80's paper, there is a signature (ie type, but in view of (1) that is implicit) TYPE of "small" types, a "universe" in type theoretic terminology.  The inhabitants of TYPE are things like nat, bool, etc as I will detail below.  (Strictly speaking, we could stipulate that the elements of TYPE are "codes" for types such as those and that there is a "decoder" that assigns a signature (ie, a type in the overall sense) to each code, but I won't bother with that here.)  The characteristic of a universe is that its elements, called "small types" or "core types", are all types in the ambient sense, which is to say signatures.  Their elements correspond to the usual things you would expect, 17, 21, true, false, etc.  In the synthetic world everything is a module, so these are in fact "modules" in the abstract sense we are developing here---thing of them as the "val" parts of what you would call a module in SML for now---it's as if [val x=3] were a "singleton module" with signature [val x:nat].  Then [type t=nat] : [type t] becomes [val t=nat] : [val t:TYPE], because the universe TYPE is a signature, and so structures defining types can be construed as providing "values" of signature TYPE.  I realize that the use of "val" here is strained, but it helps to convey the structure of the type theory.  I am simply saying that "core types" are elements (ie modules) of signature TYPE, the universe.
Again as with your 80's paper, the signatures are closed under "dependent function" and "dependent product" types, x:S1->S2(x) and x:S1 * S2(x), respectively, in their most austere formulation.  (One could have n-ary products, for example.)  When there are no actual dependencies, these degenerate to S1->S2 and S1*S2, which in the special case that the Si's are elements of the universe, ie "small types", we have automatically types such as nat -> nat and nat * nat (and, moreover, the universe is closed under these, so these continue to be small types).  But notice that we immediately have dependencies like this: t:TYPE -> t -> t, which is the type of the polymorphic identity.  This signature does not inhabit the universe TYPE, which amounts to saying that "polytypes are (functor) signatures", roughly fun t:TYPE in fun x:t in x end end, or in more ML-like notation functor (type t val x : t) : val it : t, where I have un-curried it to make the comparison easier.  The point is, ab initio, polymorphic functions are functors in the ML sense classified by dependent functor signatures.  So already we are seeing the fundamental integration of the previously disparate concepts in the language.
There is a signature of computations of a values of a small type, which is where effects such as partiality and mutation live.  Let's call that signature COMP(tau) where tau:TYPE.  It classifies run-time computations, as distinct from "plain values" of a type.  COMP(-) is a modality, called the "lax modality", that classifies encapsulated computations comp(m) where m is a computation that, when executed, yields a value of type tau (and may also have effects).  In the presence of dependency it is fundamentally important to keep values separate from computations, because (a) it can make sense (to be detailed) to have a value in a signature, eg sharing v.t = u.t, where v is a module value of a suitable signature, and (b) it does not make sense to have a computation in a signature, eg "call my grandmother then return 7" is a computation, and it makes no sense for that to appear in a type.  This is the semantic origin of generativity.  Why?  The two fundamental forms of computation are "return a value v" and "let x be m1 in m2", ie execute computations in sequence.  The latter is a little misleading, because I should specify the type of x, which may or may not reveal what are the type components of a module, rendering those that are not revealed as "fresh" precisely because the bound variable x is, by alpha-equivalence, different from all others in scope, so that x.t is different from y.t.  This is generativity.
To achieve "sealing" there is a module computation m:>s, where s is any signature, that counts as an "effect", even though it is only "pro forma" an effect when m happens to be an effect-free value.  This is how you impose abstraction.  Because a computation must be let-bound before use, then the let binding induces "fresh" types for those occurring in the signature s, which is precisely what is meant by them being "new" when the module is "sealed".  (Nice and clean, if you ask me.)  The point is that sealing is a pro forma effect, and therefore sealing with an opaque signature imposes abstraction and thus achieves "generativity".
Now comes the central concept that we introduced, the notion of a phase distinction.  There is a proposition (type with at most one element, which if present indicates that the proposition is true) STATIC, that, when inhabited, imposes equations that collapse the "dynamic" aspects of a module to a point.  This means quotienting all elements of a small type from the universe, and all elements of a computation type, so that they are all equal to each other (and to a designated element written "*" for technical reasons that don't matter here.)  The key idea is that when there is a variable/assumption that STATIC holds true, then the only thing that matters about a module are its static components, namely the types.  The hierarchical structure of modules arising from dependent product types is not affected---there is no "flattening" involved ever at all period.  All that is happening is that all of the run-time aspects of any module are collapsed to a point, leaving the compile-time aspects intact.  Why do we wish to have this concept?  Because module sharing, including type sharing, governs the static parts of modules.  (Now I know what you are going to say, and trust me, that can be handled in this framework too, without much trouble, namely structure sharing that accounts for dynamic parts.  We handle that, trust me, so don't object.)
To express sharing we introduce extent signatures of the form { M : S | STATIC --> P }, which classifies all modules of signature S whose static parts are equal to P.  So, for example, { t : TYPE | STATIC --> nat } classifies types that are equal to nat, and we can of course nest these things to express the crucial idea of "type unknown types that are constrained to be equal".  Extent types are a generalization of Stone's and Harper's singletons, but they are formulated in a way that integrates properly with everything else in this setup, especially the concept of a phase.  (SH is based on phase separation, a physical "flattening" of modules such that the static parts are constructors of a "kind", but we don't need or want that in an abstract/synthetic account of modularity.)
Finally, we may equip our language of modules with various forms of computation, eg fixed points, references, exceptions, etc.  This comes last: first modules, then core language.

And that's the overall approach!  Now it takes some work but I can sketch how, for example, datatypes and pattern matching are integrated by a combination of sealing/abstraction and recursive types.  In this view datatypes are modules with data signatures as I have outlined separately.

I will stop here to get a sense of your reading of this and what might be clarified or expanded.  I don't want to get too far ahead, but let me indicate as I mentioned above that in the second part of the SH paper, we develop the meta-theory of this language, including a representation independence theorem for abstract types, in another synthetic type theory whose types classify computability structures, which are proof-relevant generalizations of Tait's computability method.  Computability structures are governed by a phase, called SYNTAX, that isolates the syntactic object that is being said to be computable from the evidence that it is in fact computable.  Thus, computability structures are like program modules in which the "static part" are pieces of syntax and the "dynamic part" are the proofs that the syntax is "computable"!  If I may say so, it's gorgeous.  The third part of that paper is there to argue that the second part is consistent (has a model in topos theory).
