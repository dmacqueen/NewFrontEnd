..// RWH rwh-background.txt -> discussion/rwh3.txt
// Bob Harper background discussion
// 2024.12.05
// ### DBM's comments

It seems like it would be helpful to separately specify some of my suggestions, if only to
clarify what I am suggesting.

1. It is not possible to compile ML to (an imperative extension of) System F, period. 
There are several issues. One, impredicativity is totally unnecessary and
unwanted for SML. Even if it were wanted in some extension, it can be selectively
introduced in a predicate framework as may be appropriate. The claimed interpretations
into System F rely on a whole program transformation that amounts to CPS conversion,
whereas a compositional/modular account is what is necessary for (true) separate
compilation (ie, not separate type-checking, but compilation to object code). The reason
is that System F only provides types-as-arguments to terms, and cannot model modules that
produce types-as-components of structures or types-as-outputs of functors. Just forget you
ever heard about System F, and you are far better off and in any case in a more general,
flexible, expressive setting.

### I tend to agree on this point, but not with complete confidence. SML is not (based
on) F_omega. The "F-ing Modules" and "1ML" papers are about a different language.


2. An appropriate target language analogous to the aforementioned (non-)role of System F
is a core dependent type theory with one universe, that of "small" or "core language"
types. Signatures are "large" types (ie, general types, including Pi's and Sigma's), which
include the "small" types en passant. To capture ML accurately it is necessary to
introduce some means of limiting dependencies to express that in SML types cannot depend
on values. Thus, for example, dependent functor signature such as X:sig type t val x:t end
-> sig type t=X.t val x:t end used "static-on-static" dependency in which the type
components of the result depend only on the type components of the argument, there being
no "true dependency" of types on values (static-on-dynamic) in ML. Because of sealing and
the possibility of a module depending on an unknown module (as in a separate compilation
scenario), it is necessary to build into the type theory the limitation of dependency to
static-on-static form. This can be achieved in two ways, only one of which really works in
the presence of abstract types and effects, namely the notion of a "phase distinction"
introduced by H & Sterling. Briefly, there is a distinguished proposition, STATIC, a type
with at most one element indicating that it is "true", such that whenever STATIC is true
(is inhabited), then all (dynamic) values of any type are conflated to a point by
quotienting, so that they might as well be "erased". Sharing specifications between
structures would be germane only under the STATIC phase, in which case it means that the
static parts of the equated modules are to be equal. (One can express "structure sharing"
using abstract types, so it is not necessary to formulate it as a fundamental semantic
concept.)

I will not say more than this here, but I wish to emphasize that, in my terminology at
least, this sort of type theory cannot be described as "flattening" anything. Modules are
not "collapsed", they are not decomposed into separate static and dynamic components, they
are not restructured in any way under this interpretation. However, I will point out that
once one takes the dependent types with universes and a phase distinction seriously, it
becomes obvious that the "core language" concepts such as effects are obtained by
augmenting the module language just sketched with additional structure to account for
them. That is, MODULES ARE THE PRIMARY CONCEPT---the overall language design starts with
modules and then adds in core language features as may be wanted. It DOES NOT start with a
core language and somehow attempt to define modules "on top of it". Historically, that was
the thing to do, given the state of knowledge at the time and the prescient nature of
modules, but now that we know a lot about this area, the perspective I am referring to
here is the correct one; I know of no other, having studied many many variations and
possibilities over many many years.

### I'll concede that you are the expert here, and you and your students have taken the
### "theory of modules" way, way beyond where I left it in 1986.  I haven't found even
### the first section of Harper and Sterling ("Logical Relations are Types") very
### illuminating so far. It would probably take a few weeks of lectures (if there were
### a course on the topic) to bring someone like me fully up to speed on the material
### and the notations. Given what I am trying to do, I am not sure that would be the
### right thing to pursue, but the idea of having a more accessible, tutorial presentation
### of the ideas seems very appealiing.

### In my, more naive way of thinking, the product of a new front end would be a
### representation of a program in an abstract syntax that includes the information
### developed by type checkiing (at both core and module level). The abstract syntax
### used in SML/NJ seems quite deficient to me, so there is potential for significant
### improvement. I think you probably mean something rather different when you talk 
### about "An appropriate target language". Maybe you are thinking in terms of the
### experience with TILT?  I am interested in finding out what I might get from the
### TILT experience.

### In SML/NJ, the abstract syntax (such as it is) is translated to the FLINT IR
### (in particular the flint type, FLINT.flint, which is an A-normal form based
### on the F_omega like plambda intermediate language. Then a series of "middle-end"
### optimizations operate on the flint IR, The important or valuable part of FLINT
### is the exploitation of type information it its optimization passes. I think that
### a simpler, more speciallized representation would be sufficient to gain all, or
### most of the optimization results that FLINT achieves. Currently the FLINT
### optimizations are followed by further optimization phases using a CPS and then
### a "flow graph" IR.

### As an interim approach, I think it should also be possible to translate a new,
### improved abstract syntax to the FLINT IR, so that the new front end could be
### connected to the exising SML/NJ middle-end and back-end implementation.

3. It is advantageous in the "internal language" to isolate effects using either a lax
modality (as is done in H & Sterling) or a call-by-push-value separation of positive and
negative types (a more refined approach). The lax modality would have the by-now-classic
return/bind primitives for sequencing effectful computations. So return(e) is a
computation (aka command) that returns the value of the expression e, and bind(e; x.m')
evaluates expression e to cmd(m) (an encapsulated command/computation), then executes m,
and passes its value to the command m' by substitution for x. I emphasize: there is no
need to extend this distinction to the surface language! One can perfectly well integrate
effects into expressions as in ML without compromise, period. But internally, it is vital
to the needs of the type theory to draw this distinction, using either lax or cbpv
methods, as stated above. I will mention that bnd(e;x.m) can be written using
Haskell-inspired syntax as x <- e; m, which is a generalization of the familiar semicolon
from imperative programs that sequences the encapsulated command given by e before the
command m, and passes the return value of executing the command given by e to m via x. It
is thus a generalization of the "plain semicolon" we are now using to our disadvantage. In
surface terms this could be written x <- e; e', and have essentially the same meaning as
let x be e in e' end, with the understanding that this let engenders effects and imposes
sequencing.

### Again, I am not sure what you mean by the "internal language" (TILT?). If such
### an approach were taken, I assume the new typed abstract syntax would be
### the starting point for translation to such an "internal language", which would
### thus be analagous to the FLINT middle-end.


4. It is a fallacy that ML-style type inference during elaboration has anything at all to
do with type abstraction in the internal language. I can elaborate on that if you wish.
Unfortunately 35 years ago I encouraged the interpretation I am now discouraging, as I
have learned more over many years. I will note that in the IL described in (2), so-called
"explicit polymorphism" is none other than a functor type, t : TYPE -> VAL(t->t), for
example, where TYPE is the universe of core language types, and VAL(tau), or just tau,
classifies expressions of that type. That is, polymorphic functions ARE functors in our
sense, period. There is not and need not be any other form of polymorphism, and certainly
not impredicative quantification as found in System F. This is the spot where type classes
fit in naturally, replacing t:TYPE above with X:[t : TYPE, eq : t * t -> bool], et voila
you have "equality types", etc.

### Again, the problem is: What is the "internal language" is and what role does it pley
### in compilation, and how does it relate to the abstract syntax I plan to produce?


5. My suggestion regarding continuations was off-hand, but here it is: add a new form of
"bind" in ML, written "cc k", so that let cc k in ... end would be an expression that,
when evaluated, binds the "current continuation" to the variable k of type tau cont for
use in the ellided part, which can "throw" to k as a transfer of control to the spot where
the "cc k" occurred. It would take some thought to give this a sensible semantics
(including local cc k in ... end and struct cc k ... end), and maybe it's a bad idea, I'm
not sure. What I care about, though, is that continuations be embraced, not denigrated,
and integrated into the language.


6. I fully agree that some
sort of pre-declaration of the types of val's is useful (and essential in the recursive
polymorphic case) it is not clear to me what exactly is the proposal. Whereas let val x :
int val x = 3 in .. end seems to make sense, let val x : int in .. end does not make
sense.

### Here I agree that the "proposal" is rather vague, and the syntax is not yet worked out.


7. My proposal of datatypes-as-modules is what we do in TILT. I haven't worked it out
fully as an elaboration of ML into an internal language, but the high-level idea is that
so-called concrete datatype's are in fact abstract types in the precise sense that (a) the
actual implementation is hidden, (b) values are created by constructors and decomposed by
pattern matching/case analysis on that abstract type, and (c) abstraction is achieved
using "sealing" of modules. So, concretely, a datatype spec turns into what I'm calling a
"data signature", which looks like this: NAT = data type t con zero : t con succ t->t end,
and then there is a special module declaration data structure N :> NAT, full stop (no "=",
but see below). The compiler generates the structure N based on the data signature NAT,
which tells it all it needs to know to support pattern matching and constructing values of
the type t. Just at the level of syntax this approach eliminates the vexed duplication of
datatype's problem (once in the signature, once in the structure) that we all hate, and it
integrates with modules properly. Note that if you wish, as I think you do, to force
datatypes "upward" to the module level, and remove them from the core level, then this
approach is ideal because then datatype's exist only as data structure declarations for a
given data signature. Done. If, on the other hand, you realize that imposing the
separation of modules from core is a wrong-headed artifact of the development phase of the
ideas, then it's fine too, because allowing "let data structure N :> NAT in ... end" is no
different than allowing "let structure Mine = ... in ... end", and we're good. The latter
is the right thing to do, but the proposal is compatible with the former.

### Again, your "internal language" is probably not what I have in mind as the abstract
### syntax that the front-end should produce.


8. Believe it or not, the Twelf code is quite readable! Have a look. This is certainly the
right way to maintain a definition of a language. Any change you make to the language
itself automatically tells you all and only the spots you have to review in order to
accommodate the extension. Why? Because it's all done by (higher-order) pattern matching,
exactly as ML uses patterns and thereby provides a very nice way to chase down the effects
of changing a data type, so incredibly useful it's amazing. It's the same for Twelf. It
shows you where the safety proof is missing cases, or where some argument no longer goes
through. There are no "tactics" or anything to obscure what is going on: all proofs are
done by pattern matching a la ML, with a directive to ensure that it's a valid inductive
proof (covers all cases, is not circular).

### Sounds good in principle, but it would be nice to have an explanatory document,
### perhaps analagous to the "Commentary" volume for the 1990 Defintion to explain the
### formalization approach and how to read the TWELF specification and derive answers
### to specific questions from it. But having a "mechanized" formalization that can
### maintained and evolved sounds great. The existing Definition, being based on
### techniques and knowledge dating from the mid 1980s (hence 40 years old!) should
### clearly be considered obsolete!


------------------------------------------------------------------------------------------
### Here is a thought!  The current SML/NJ abstract syntax is ad hoc
### and incomplete.  There in fact is no proper abstract syntax for modules.
### Perhaps you can help with this part of the new abstract syntax, capturing
### the information that is needed by a middle-end as client, either by translation
### the old FLINT _or_ by translation to the sort of "internal language" you have in
### mind. But with the requirement that eventually we'll produce something that
### can be used by an existing back end (of which there are currently two: MLRISC-based
### and LLVM-based).
