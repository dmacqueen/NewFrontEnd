// RWH rwh-background.txt -> discussion/rwh3.txt
// Bob Harper background discussion
// 2024.12.05

It seems like it would be helpful to separately specify some of my suggestions, if only to clarify what I am suggesting.

1. It is not possible to compile ML to (an imperative extension of) System F, period.  This is a fallacy of long standing that has been encouraged by the same fallacy often stated about Haskell.  There are several issues.  One, impredicativity is totally unnecessary and unwanted for SML.  Even if it were wanted in some extension, it can be selectively introduced in a predicate framework as may be appropriate.  The claimed interpretations into System F rely on a whole program transformation that amounts to CPS conversion, whereas a compositional/modular account is what is necessary for (true) separate compilation (ie, not separate type-checking, but compilation to object code).  The reason is that System F only provides types-as-arguments to terms, and cannot model modules that produce types-as-components of structures or types-as-outputs of functors.  Just forget you ever heard about System F, and you are far better off and in any case in a more general, flexible, expressive setting.

2. An appropriate target language analogous to the aforementioned (non-)role of System F is a core dependent type theory with one universe, that of "small" or "core language" types.  Signatures are "large" types (ie, general types, including Pi's and Sigma's), which include the "small" types en passant.  To capture ML accurately it is necessary to introduce some means of limiting dependencies to express that in SML types cannot depend on values.  Thus, for example, dependent functor signature such as X:sig type t val x:t end -> sig type t=X.t val x:t end used "static-on-static" dependency in which the type components of the result depend only on the type components of the argument, there being no "true dependency" of types on values (static-on-dynamic) in ML.  Because of sealing and the possibility of a module depending on an unknown module (as in a separate compilation scenario), it is necessary to build into the type theory the limitation of dependency to static-on-static form.  This can be achieved in two ways, only one of which really works in the presence of abstract types and effects, namely the notion of a "phase distinction" introduced by H & Sterling.  Briefly, there is a distinguished proposition, STATIC, a type with at most one element indicating that it is "true", such that whenever STATIC is true (is inhabited), then all (dynamic) values of any type are conflated to a point by quotienting, so that they might as well be "erased".  Sharing specifications between structures would be germane only under the STATIC phase, in which case it means that the static parts of the equated modules are to be equal.  (One can express "structure sharing" using abstract types, so it is not necessary to formulate it as a fundamental semantic concept.)  

I will not say more than this here, but I wish to emphasize that, in my terminology at least, this sort of type theory cannot be described as "flattening" anything.  Modules are not "collapsed", they are not decomposed into separate static and dynamic components, they are not restructured in any way under this interpretation.  However, I will point out that once one takes the dependent types with universes and a phase distinction seriously, it becomes obvious that the "core language" concepts such as effects are obtained by augmenting the module language just sketched with additional structure to account for them.  That is, MODULES ARE THE PRIMARY CONCEPT---the overall language design starts with modules and then adds in core language features as may be wanted.  It DOES NOT start with a core language and somehow attempt to define modules "on top of it".  Historically, that was the thing to do, given the state of knowledge at the time and the prescient nature of modules, but now that we know a lot about this area, the perspective I am referring to here is the correct one; I know of no other, having studied many many variations and possibilities over many many years.

3. It is advantageous in the "internal language" to isolate effects using either a lax modality (as is done in H & Sterling) or a call-by-push-value separation of positive and negative types (a more refined approach).  The lax modality would have the by-now-classic return/bind primitives for sequencing effectful computations.  So return(e) is a computation (aka command) that returns the value of the expression e, and bind(e; x.m') evaluates expression e to cmd(m) (an encapsulated command/computation), then executes m, and passes its value to the command m' by substitution for x.  I emphasize: there is no need to extend this distinction to the surface language!  One can perfectly well integrate effects into expressions as in ML without compromise, period.  But internally, it is vital to the needs of the type theory to draw this distinction, using either lax or cbpv methods, as stated above.  I will mention that bnd(e;x.m) can be written using Haskell-inspired syntax as x <- e; m, which is a generalization of the familiar semicolon from imperative programs that sequences the encapsulated command given by e before the command m, and passes the return value of executing the command given by e to m via x.  It is thus a generalization of the "plain semicolon" we are now using to our disadvantage.  In surface terms this could be written x <- e; e', and have essentially the same meaning as let x be e in e' end, with the understanding that this let engenders effects and imposes sequencing.

4. It is a fallacy that ML-style type inference during elaboration has anything at all to do with type abstraction in the internal language.  I can elaborate on that if you wish.  Unfortunately 35 years ago I encouraged the interpretation I am now discouraging, as I have learned more over many years.  I will note that in the IL described in (2), so-called "explicit polymorphism" is none other than a functor type, t : TYPE -> VAL(t->t), for example, where TYPE is the universe of core language types, and VAL(tau), or just tau, classifies expressions of that type.  That is, polymorphic functions ARE functors in our sense, period.  There is not and need not be any other form of polymorphism, and certainly not impredicative quantification as found in System F.  This is the spot where type classes fit in naturally, replacing t:TYPE above with X:[t : TYPE, eq : t * t -> bool], et voila you have "equality types", etc.

5. My suggestion regarding continuations was off-hand, but here it is: add a new form of "bind" in ML, written "cc k", so that let cc k in ... end would be an expression that, when evaluated, binds the "current continuation" to the variable k of type tau cont for use in the ellided part, which can "throw" to k as a transfer of control to the spot where the "cc k" occurred.  It would take some thought to give this a sensible semantics (including local cc k in ... end and struct cc k ... end), and maybe it's a bad idea, I'm not sure.  What I care about, though, is that continuations be embraced, not denigrated, and integrated into the language.

6. I fully agree that some sort of pre-declaration of the types of val's is useful (and essential in the recursive polymorphic case) it is not clear to me what exactly is the proposal.  Whereas let val x : int val x = 3 in .. end seems to make sense, let val x : int in .. end does not make sense.

7. My proposal of datatypes-as-modules is what we do in TILT.  I haven't worked it out fully as an elaboration of ML into an internal language, but the high-level idea is that so-called concrete datatype's are in fact abstract types in the precise sense that (a) the actual implementation is hidden, (b) values are created by constructors and decomposed by pattern matching/case analysis on that abstract type, and (c) abstraction is achieved using "sealing" of modules.  So, concretely, a datatype spec turns into what I'm calling a "data signature", which looks like this: NAT = data type t con zero : t con succ t->t end, and then there is a special module declaration data structure N :> NAT, full stop (no "=", but see below).  The compiler generates the structure N based on the data signature NAT, which tells it all it needs to know to support pattern matching and constructing values of the type t.  Just at the level of syntax this approach eliminates the vexed duplication of datatype's problem (once in the signature, once in the structure) that we all hate, and it integrates with modules properly.  Note that if you wish, as I think you do, to force datatypes "upward" to the module level, and remove them from the core level, then this approach is ideal because then datatype's exist only as data structure declarations for a given data signature.  Done.  If, on the other hand, you realize that imposing the separation of modules from core is a wrong-headed artifact of the development phase of the ideas, then it's fine too, because allowing "let data structure N :> NAT in ... end" is no different than allowing "let structure Mine = ... in ... end", and we're good.  The latter is the right thing to do, but the proposal is compatible with the former.

8. Believe it or not, the Twelf code is quite readable!  Have a look.  This is certainly the right way to maintain a definition of a language.  Any change you make to the language itself automatically tells you all and only the spots you have to review in order to accommodate the extension.  Why?  Because it's all done by (higher-order) pattern matching, exactly as ML uses patterns and thereby provides a very nice way to chase down the effects of changing a data type, so incredibly useful it's amazing.  It's the same for Twelf.  It shows you where the safety proof is missing cases, or where some argument no longer goes through.  There are no "tactics" or anything to obscure what is going on: all proofs are done by pattern matching a la ML, with a directive to ensure that it's a valid inductive proof (covers all cases, is not circular).

